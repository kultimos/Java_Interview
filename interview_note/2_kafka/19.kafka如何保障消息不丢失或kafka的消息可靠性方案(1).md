# kafka如何保障消息不丢失或kafka的消息可靠性方案 
  - Producer端
    需要确保消息能够到达Broker,并且实现消息的成功写入,而在这个过程中有可能出现网络问题导致消息发送失败,所以针对Producer有两种方式来避免消息丢失
    1)Producer默认是异步发送消息,可以通过将异步发送改成同步发送来保证Producer可以实时知道消息发送的结果
    2)仍然异步发送消息,但添加异步回调函数来监听消息发送结果,如果发送失败,可以在回调中重试,基于此也可以避免消息丢失
    3)Producer本身提供了一个重试配置参数,retries,如果设置一个大于零的值,则表示生产者会在遇到可恢复性错误时按预设值多次尝试重新发送消息,直到成功或达到指定的重试次数为止;
  如果因为网络问题或Broker故障导致发送失败,那么Producer会自动重试
    4)配置事务,确保一条消息不会被重复发送,即使发送的过程中出现了故障,kafka的事务只针对于生产者

  - Broker端
    Broker需要确保Producer发送来的消息不会丢失,也就是说只需要去把这个消息持久化到磁盘就可以了;但是kafka为了提升性能采用了异步批量刷盘的实现机制,
  也就是按照一定的消息量和时间间隔去刷盘,而最终刷新到磁盘这个动作是由操作系统来调度的,所以如果在刷盘之前系统崩溃了就会导致数据丢失,kafka并没有提供
  同步刷盘的实现机制,所以针对这个问题需要通过partition的副本机制和ack机制来解决;即kafka可以通过上设置ack和副本因子相关参数来通过多节点的写入确认
  来保障数据的可靠性,参数有replication.factor、acks和min.insync.replica,[原理见:](8.kafka遵循着cap原则吗.md)

  - Consumer端
    消费端可能存在一种异步业务逻辑的情况,如果业务逻辑异步进行,而消费者已经同步提交了offset,那么如果业务逻辑执行异常,但Broker已经收到了消费者应答,
  更新了offset,从业务的角度,这也是一次消息的丢失;
    但即便出现这样一种情况,我们也可以通过重新调整offset的值来实现重新消费;